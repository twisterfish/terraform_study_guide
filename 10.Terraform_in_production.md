 
# Module 10 - Production-Grade Terraform Code
---

---

## What's Involved?

* Servers
* Data stores
* Load balancers
* Security
* Monitoring and alerting tools
* Building pipelines
* All the other pieces of your technology that are necessary to run a business

## Goals

Your infrastructure won't fall over if traffic goes up
* Not lose your data if there's an outage
* Not allow that data to be compromised when hackers try to break in
* If that is not achieved then a company can go out of business.


## Estimates

Deploy a service fully managed by a third party 
- Such as running MySQL using the AWS Relational Database Service (RDS)
- __One to two weeks__ to get that service ready for production

Your own stateless distributed app
- Such as a cluster of Node.js apps
- They store all their data in RDS
- On top of an AWS Auto Scaling Group (ASG)
- About __two to four weeks__ to get ready for production

---

## Bigger Estimates

Your own stateful distributed app
* Such as an Amazon Elasticsearch (Amazon ES) cluster
* On top of an ASG and stores data on local disks
* __Two to four months__ to get ready for production

You need to build out your entire architecture including
* All of your apps 
* Data stores
* Load balancers
* Monitoring
* Alerting
* Security
  * About __6 to 36 months__ of work
    * Companies typically closer to six months
    * Larger companies typically taking several years

![](images/estimate-summary.png?raw=true)

---

## Production Grade Infrastructure

Why does it take so long to build production-grade infrastructure?

The production-grade infrastructure checklist

Production-grade infrastructure modules
- Small modules
- Composable modules
- Testable modules
- Releasable modules


## Why it Takes a Long Time

Software estimates are notoriously inaccurate.
- Even worse for DevOps
- Unforeseen problems occur that disrupt the process

The industry is in its infancy
- Cloud computing, DevOps and tools like Terraform are all recent innovations
- Still changing rapidly and innovating - not stable yet
- Most people do not have a depth of experience with them

The process is prone to disruption
- The integration of all the details into a smooth process is still not stable
- Diagnostic methods to solve problems are still maturing
- There does not yet exist a body of experience and knowledge that covers a wide range of situations


## The Complexity Problem

Scaling up from simple environments where everything works easily to complex environments is always problematic
- Complexity is a major cause of software failure

**Essential complexity** is when we are working with a complex problem - like building a whole corporate IT infrastructure

**Accidental complexity** arises from the problems involved in using a specific set of tools and processes -- wrong tools and processes made the solution difficult

**Environmental complexity** comes from disorganized work environments such as when there is a lack of processes or leadership

Not being able to manage complexity effectively can often impact a project's timelines and costs as developers are overwhelmed by a flood of details


## Infrastructure Checklist

A major challenge is that different groups in a company have only a partial view of "going to production" based on their own focus of activity

Infrastructure Checklist

![](images/checklist01.png?raw=true)


![](images/checklist02.png?raw=true)


![](images/checklist03.png?raw=true)


![](images/checklist04.png?raw=true)

---

## Production Grade Modules

Properties of production grade Terraform modules:
* Small modules
* Composable modules
* Testable modules
* Releasable modules
* Beyond Terraform modules

---
## Small Modules

A well-known anti-pattern "the Kitchen Sink Module"
* All code gets dumped into a single module
* It just sort of "grows"
* Often the root module

Large modules ( > several hundred lines of code) have downsides - they are:
* **Slow** - the plan phase takes a long time to execute
* **Insecure** - fine-grained permissions on resources becomes almost impossible
* **Risky** - a single error can propagate across the infrastructure
* **Difficult to understand** - they are a wall of (usually disorganized) text
* **Difficult to review** - not just to read, but the plan output is overwhelming to read
* **Difficult to test** - testing is hard enough already

---

## General Design Principles

In engineering, there are three basic design principles:

**Modularity**: systems are organized in self-contained packages or modules

**Cohesion**: each module should provide one service and only one module provides each service

**Coupling**: The connections between modules are only through defined interfaces

These ideas should be implemented in Terraform production modules.

---

## Architecture Example

Given the following complex AWS architecture:

![](images/aws-arch-example.png?raw=true)


This suggests a module structure like this:

![](images/aws-arch-example-2.png?raw=true)


## Refactoring Analysis

The webserver cluster file manages three resources
* Auto Scaling Group (ASG)
* Application Load Balancer (ALB)
* Hello, World app

Refactoring is the process of changing the structure or organization of software without altering its functionality - like these smaller, cohesive modules
* *modules/cluster/asg-rolling-deploy* - A generic, reusable, standalone module for deploying an ASG that can do a zero-downtime, rolling deployment
* *modules/networking/alb* - A generic, reusable, standalone module for deploying an ALB
* *modules/services/hello-world-app* - A module specifically for deploying the "Hello, World" app



## Composable Modules
Coupling requires we pass information between modules through interfaces
* In Terraform, we do this by passing data through variables

The core idea is to minimize side effects
* Modules should avoid reading state information directly from the environment
* Instead, state information is passed via input parameters
* Essentially, there are "slots" in the module which are filled in with the information in the input variables

Modules that only produce a side effect and do not return a are difficult to use
* If a module creates a resource, it should return a reference to the resource

---

## Refactoring - Defining Variables

In order to improve reuse for modules, input variables pass all the information the module needs to perform its function. For example, for the ASG Module:

![](images/InterfaceVariables.png?raw=true)

---

## Understanding the Variables

_subnet_ids_: The original code may have hard coded the default VPC and subnets, but by using this variable instead, the code can be used with any VPC or subnets

_target_group_arns_: configures how the auto-scaling group integrates with load balancers

_health_check_type_: also configures how the auto-scaling group integrates with load balancers

Instead of the code just implementing as single fixed configuration, the variables allows you to use the ASG with a wide variety of use cases; e.g., no load balancer, one ALB, multiple NLBs, and so on


## Modifying the Module Interface

Once the variables are defined, the ASG module is modified tu use the variables instead of any hardcoded resource references.

![](images/ASG-Rewrite.png?raw=true)



## The User Script

The fourth variable, `user_data`, is for passing in a User Data script

This allows us to deploy any application across an ALB

![](images/ALGModule.png?raw=true)


## Output Variables

You'll also want to add a couple of useful output variables to `modules/cluster/asg-rolling-deploy/outputs.tf`

Outputting this data makes the asg-rolling-deploy module even more reusable

Users of the module can add new behaviors, such as attaching custom rules to the security group
  
![](images/ASGOutput.png?raw=true)

For similar reasons, several output variables can be added to `modules/networking/alb/outputs.tf`:
* We have defined return values that provide the name, arn and security group id that were created when the resources were instantiated

![](../artwork/ASGOutput2.png)


## Creating the "Hello World" App

The last step is to convert the `webserver-cluster` module into a hello-world-app module that can deploy a "Hello, World" app using the `asg-rolling-deploy` and `alb` modules

The resources left in `module/services/hello-world-app/main.tf` are:
 * _template_file_ (for User Data)
 * _aws_lb_target_group_
 * _aws_lb_listener_rule_
 * _terraform_remote_state_ (for the DB)
 * _aws_vpc_
 * _aws_subnet_ids_

Add the following variable to _modules/services/hello-world-app/variables.tf_:

![](images/ASGVars.png?raw=true)


## Adding the _asg-rolling-deploy_ Module

Now, add the asg-rolling-deploy module that you created earlier to the hello-world-app module to deploy an ASG:

 ![](images/ASGMod.png?raw=true) 


## Adding the _alb_ Module

Use of the input variable environment to enforce a naming convention, so all of your resources will be namespaced based on the environment (e.g., hello-world-stage, hello-world-prod)

![](images/ModAlb.png?raw=true)


## Configure the ALB Target Group

Configure the ALB target group and listener rule for this app
* Update the aws_lb_target_group resource in _modules/services/hello-world-app/main.tf_ to use environment in its name:

![](images/ALBTargetGroup.png?raw=true)


## Update Listener

 Update the listener_arn parameter of the aws_lb_listener_rule resource to point at the alb_http_listener_arn output of the ALB module:

 ![](images/ListenerGroup.png?raw=true)


## Pass Through Values

Pass through the important outputs from the _asg-rolling-deploy_ and _alb_ modules as outputs of the hello-world-app module:

![](images/Passthrough.png?raw=true)


## Module Composition

Composition is building up more complicated behavior for the "Hello, World" app from simpler parts (ASG and ALB modules)

A fairly common pattern in terraform is that a configuration will have at least two types of modules:
* **Generic modules**: the basic building blocks of terraform code, reusable across a wide variety of use cases
* **Use-case-specific modules**: Combines multiple generic modules with some specific "glue" code to serve one specific use case such as deploying the "Hello, World" app

## Testable Modules

 There is a lot of code in three modules:
* _asg-rolling-deploy_
* _alb_
* _hello-world-app_

The next step is to check that your code actually works.
* These are not root modules, so we have to provide the infrastructure needed to run them
* For example: backend configuration, provider, etc

To do this, we use an "examples" folder that provides examples on how to use the module


## Sample Example

The code below is an example file to deploy an ASG of size 1

![](images/ExampleCode1.png?raw=true)


## Test Assets

The code above provides:

A manual test harness: you can use this example code to repeatedly deploy and undeploy manually to check that it works as you expect

An automated test harness: The example code is also creates a framework for using automated testing tools

Executable documentation: If you commit an example (including README.md) into version control then:
* Other team members can use it to understand how the module works
* They can try out the module without writing a line of code.

Every module should have a corresponding example in the examples folder
* There may be multiple examples showing different configurations and permutations of how that module can be used


## Folder Structure

```
modules
 └ examples
   └ alb
   └ asg-rolling-deploy
     └ one-instance
     └ auto-scaling
     └ with-load-balancer
     └ custom-tags
   └ hello-world-app
   └ mysql
 └ modules
   └ alb
   └ asg-rolling-deploy
   └ hello-world-app
   └ mysql
 └ test
   └ alb
   └ asg-rolling-deploy
   └ hello-world-app
   └ mysql
```

## Version Pinning

You should pin all of your Terraform modules to a specific version of Terraform using the required_version argument
*  You want to avoid breaking a configuration because of a change in terraform itself
*  At a bare minimum, you should require a specific major version of terraform:
* For production-grade code, it is recommended pinning the version even more strictly:

![](images/versionpinning.png?raw=true)



## Releasable Modules

Modules should be made available for use only after they have been "released"

Another option for releasing modules is to publish them in the Terraform Registry
* The Public Terraform Registry resides at https://registry.terraform.io/
* It includes hundreds of reusable, community-maintained, open source modules for AWS, Google Cloud, Azure, and many other providers

## Publishing Requirements

There are a few requirements to publish a module to the Public Terraform Registry
* The module must live in a public GitHub repo
* The repo must be named terraform-< PROVIDER >-< NAME >, where PROVIDER is the provider the module is targeting (e.g., aws) and NAME is the name of the module
* The module must follow a specific file structure, including defining Terraform code in the root of the repo, providing a README.md, and using the convention of main.tf, variables.tf, and outputs.tf as filenames
* The repo must use Git tags with semantic versioning (x.y.z) for releases

## Using the Registry

Terraform even supports a special syntax for consuming modules from the Terraform Registry
* You can use a special shorter registry URL in the source argument and specify the version via a separate version argument using the following syntax:

![](images/registry.png?raw=true)


## Beyond Modules

Sometimes non Terraform code has to be run from a Terraform module or integrate with other tools or systems

Sometimes we have to work around a limitation of Terraform, like trying implement complicated logic

Some Terraform "escape hatches" are:
* Provisioners
* Provisioners with _null_resource_
* External Data Source


## Provisioners

Provisioners are used to execute scripts either on the local machine or a remote machine, typically to do the work of bootstrapping, configuration management, or cleanup

There are several types of provisioners
* _local-exec_: execute a script on the local machine
* _remote-exec_: execute a script on a remote resource
* _chef_: run Chef client on a remote resource
* _file_: copy files to a remote resource
  

## Provisioner Block

Provisioners are added using the a `provisioner block`

![](images/localexec1.png?raw=true)

* Running `terraform apply` produces:

![](images/local-exec-1.png?raw=true)



## Remote Provisioning

Assume we want to provision an EC2 instance, we have to connect to the instance and authenticate to it
* In this example we will use SSH
* We need a security group to allow SSH access:

![](images/ex-6-2-1.png?raw=true)

## Generate SSH Keys

This stores the key in the state which we would not want to do in a production environment and upload to aws using the `aws_key_pair` resource

![](images/ex-6-2-2.png?raw=true)


## Creating and Connecting to the Instance 

The inline argument to pass a list of commands to execute, instead of a single command argument
* But we also have to configure the EC2 instance to use ssh
 * The `self` keyword is a work-around for provisioners to avoid circular dependencies.

 * ![](images/ex-6-2-3.png?raw=true)
   

## Output

Running _terraform apply_ produces:

![](images/ex6-2-output.png?raw=true)



## Data Scripts versus Provisioners

Advantages to using a provisioner
* Data scripts are limited to a length of 16KB, while provisioner scripts can be arbitrarily long
* Chef and other provisioners install, configure, and run on clients, which makes it easier to use configuration management tools

The advantages to User Data scripts are:
* You can use User Data scripts with ASGs, but Provisioners take effect only while Terraform is running and don't work with ASGs at all
* The User Data script can be seen in the EC2 Console and you can find its execution log on the EC2 Instance itself, both of which are useful for debugging,neither of which is available with provisioners










## Provisioners with "null_resource"

* Sometimes, you want to execute a provisioner without tying it to a specific resource
* we can use a _null_resource_ which acts just like a normal terraform resource, except that it does not create anything
 * The _null_resource_ has an argument called triggers, which takes in a map of keys and values
  * Whenever the values change, the null_resource will be recreated
  * This forces any provisioners within it to be re-executed
  * For example, the uuid() built-in function, which returns a new, randomly generated UUID each time it's called, within the triggers argument

 ![](../artwork/ex-6-3.png) 


Notes:

This is example 6-3

---
## Output from _null_resource_

* Every time you call terraform apply, the local-exec provisioner will execute:
* The output from the previous example is:

![](../artwork/ex-6-3-output.png)

     
---

## External Data Source

* For situations where  we want to execute a script to fetch some data and make that data available within the Terraform code itself
  * External data source allows an external command that implements a specific protocol to act as a data source

* The protocol is:
  * Data passes terraform to the external program using the query argument of the external data source
  * The external program can read in these arguments as JSON from stdin
  * The external program can pass data back to Terraform by writing JSON to stdout
  * Terraform code can then pull data out of this JSON by using the result output attribute of the external data source


---
## External Example

* This example uses the external data source to execute a Bash script that echoes back to stdout any data it receives on stdin

![](../artwork/ex-6-4.png)
  
---


